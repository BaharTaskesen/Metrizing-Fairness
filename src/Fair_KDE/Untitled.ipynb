{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "constant-easter",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-66b22ba7e15e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfairness_metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdata_loader_or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Projects\\Fairness\\fair_regression_Lp\\Fair_KDE\\data_loader_or.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mload_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'load_data'"
     ]
    }
   ],
   "source": [
    "# Baseline Fair KDE : https://proceedings.neurips.cc//paper/2020/file/ac3870fcad1cfc367825cda0101eee62-Paper.pdf\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import fairness_metrics\n",
    "import data_loader_or\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from sklearn.metrics import log_loss\n",
    "from copy import deepcopy\n",
    "import os, sys\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import Classifier\n",
    "from dataloader import FairnessDataset\n",
    "from algorithm import train_fair_classifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import CustomDataset\n",
    "from utils import measures_from_Yhat\n",
    "\n",
    "tau = 0.5\n",
    "\n",
    "# Approximation of Q-function given by López-Benítez & Casadevall (2011) based on a second-order exponential function & Q(x) = 1- Q(-x):\n",
    "a = 0.4920\n",
    "b = 0.2887\n",
    "c = 1.1893\n",
    "Q_function = lambda x: torch.exp(-a*x**2 - b*x - c) \n",
    "\n",
    "def CDF_tau(Yhat, h=0.01, tau=0.5):\n",
    "    m = len(Yhat)\n",
    "    Y_tilde = (tau-Yhat)/h\n",
    "    sum_ = torch.sum(Q_function(Y_tilde[Y_tilde>0])) \\\n",
    "           + torch.sum(1-Q_function(torch.abs(Y_tilde[Y_tilde<0]))) \\\n",
    "           + 0.5*(len(Y_tilde[Y_tilde==0]))\n",
    "    return sum_/m\n",
    "\n",
    "def Huber_loss(x, delta):\n",
    "    if x.abs() < delta:\n",
    "        return (x ** 2) / 2\n",
    "    return delta * (x.abs() - delta / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spiritual-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act on experiment parameters:\n",
    "data_loader.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "powered-motion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COMPAS dataset...\n",
      "\n",
      "Number of people recidivating within two years\n",
      "0    2795\n",
      "1    2483\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Features we will be using for classification are: ['age_cat_25 - 45', 'age_cat_Greater than 45', 'age_cat_Less than 25', 'sex', 'priors_count', 'c_charge_degree'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma_candidates = np.logspace(-2, 2, num=10)\n",
    "\n",
    "ds = data_loader.Compas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "genuine-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.split_test()\n",
    "k = ds.get_k()\n",
    "\n",
    "metrics = {\n",
    "    'statistical_parity' : fairness_metrics.statistical_parity,\n",
    "    'statistical_parity_classification' : fairness_metrics.statistical_parity_classification,\n",
    "    'bounded_group_loss_L1' : lambda y1_hat, y2_hat, y1, y2: fairness_metrics.bounded_group_loss(y1_hat, y2_hat, y1, y2, loss='L1'),\n",
    "    'bounded_group_loss_L2' : fairness_metrics.bounded_group_loss,\n",
    "    'group_fair_expect' : fairness_metrics.group_fair_expect,\n",
    "    'l1_dist' : lambda y1_hat, y2_hat, y1, y2: fairness_metrics.lp_dist(y1_hat, y2_hat, y1, y2, p=1),\n",
    "    'l2_dist' : lambda y1_hat, y2_hat, y1, y2: fairness_metrics.lp_dist(y1_hat, y2_hat, y1, y2, p=2),\n",
    "    'MSE' : fairness_metrics.MSE,\n",
    "    'MAE' : fairness_metrics.MAE,\n",
    "    'accuracy' : fairness_metrics.accuracy\n",
    "}\n",
    "# storage of results\n",
    "results_train = []\n",
    "results_test = []\n",
    "dataset_name = 'COMPAS' # ['Moon', 'Lawschool', 'AdultCensus', 'CreditDefault', 'COMPAS']\n",
    "\n",
    "##### Which fairness notion to consider (Demographic Parity / Equalized Odds) #####\n",
    "fairness = 'DP' # ['DP', 'EO']\n",
    "\n",
    "##### Model specifications #####\n",
    "n_layers = 2 # [positive integers]\n",
    "n_hidden_units = 16 # [positive integers]\n",
    "\n",
    "##### Our algorithm hyperparameters #####\n",
    "h = 0.1 # Bandwidth hyperparameter in KDE [positive real numbers]\n",
    "delta = 1.0 # Delta parameter in Huber loss [positive real numbers]\n",
    "lambda_ = 0.05 # regularization factor of DDP/DEO; Positive real numbers \\in [0.0, 1.0]\n",
    "\n",
    "##### Other training hyperparameters #####\n",
    "batch_size = 2048\n",
    "lr = 2e-4\n",
    "lr_decay = 1.0 # Exponential decay factor of LR scheduler\n",
    "n_seeds = 5 # Number of random seeds to try\n",
    "n_epochs = 200\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "##### Whether to enable GPU training or not\n",
    "device = torch.device('cpu') # or torch.device('cpu')\n",
    "# Import dataset\n",
    "# dataset = FairnessDataset(dataset=dataset_name, device=device)\n",
    "# dataset.normalize()\n",
    "input_dim = k + 1\n",
    "\n",
    "net = Classifier(n_layers=n_layers, n_inputs=input_dim, n_hidden_units=n_hidden_units)\n",
    "net = net.to(device)\n",
    "\n",
    "# Set an optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay) # None\n",
    "\n",
    "#     X, Y, A = ds.get_data()\n",
    "#     X_test, Y_test, A_test = ds.get_test_data()\n",
    "#     x_train = X.cpu().detach().numpy()\n",
    "#     Y_train = Y.cpu().detach().numpy().flatten()\n",
    "#     a_train = A.cpu().detach().numpy().flatten()\n",
    "#     x_test = X_test.cpu().detach().numpy()\n",
    "#     y_test = Y_test.cpu().detach().numpy().flatten()\n",
    "#     a_test = A_test.cpu().detach().numpy().flatten()\n",
    "\n",
    "# train_tensors, test_tensors = dataset.get_dataset_in_tensor()\n",
    "# X_train, Y_train, Z_train, XZ_train = train_tensors\n",
    "# X_test, Y_test, Z_test, XZ_test = test_tensors\n",
    "\n",
    "# Retrieve train/test splitted numpy arrays for index=split\n",
    "# train_arrays, test_arrays = dataset.get_dataset_in_ndarray()\n",
    "# X_train_np, Y_train_np, Z_train_np, XZ_train_np = train_arrays\n",
    "# X_test_np, Y_test_np, Z_test_np, XZ_test_np = test_arrays\n",
    "\n",
    "X_train, Y_train, Z_train = ds.get_data()\n",
    "X_test, Y_test, Z_test = ds.get_test_data()\n",
    "XZ_test = torch.cat([X_test, Z_test], 1)\n",
    "XZ_train = torch.cat([X_train, Z_train], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abstract-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(XZ_train, Y_train, Z_train)\n",
    "if batch_size == 'full':\n",
    "    batch_size_ = XZ_train.shape[0]\n",
    "elif isinstance(batch_size, int):\n",
    "    batch_size_ = batch_size\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_size_, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "infrared-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = torch.tensor(np.pi).to(device)\n",
    "phi = lambda x: torch.exp(-0.5*x**2)/torch.sqrt(2*pi) #normal distribution\n",
    "\n",
    "# # An empty dataframe for logging experimental results\n",
    "# df = pd.DataFrame()\n",
    "# df_ckpt = pd.DataFrame()\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "generic-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Batch [1/1], Cost: 0.6302\r"
     ]
    }
   ],
   "source": [
    "n_epochs = 6\n",
    "results_test = []\n",
    "results_train = []\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (xz_batch, y_batch, z_batch) in enumerate(data_loader):\n",
    "        xz_batch, y_batch, z_batch = xz_batch.to(device), y_batch.to(device), z_batch.to(device)\n",
    "        Yhat = net(xz_batch)\n",
    "        Ytilde = torch.round(Yhat.squeeze())\n",
    "        cost = 0\n",
    "        dtheta = 0\n",
    "        m = z_batch.shape[0]\n",
    "\n",
    "        # prediction loss\n",
    "        p_loss = loss_function(Yhat.squeeze(), y_batch.squeeze())\n",
    "        cost += (1 - lambda_) * p_loss\n",
    "\n",
    "        # DP_Constraint\n",
    "        if fairness == 'DP':\n",
    "            Pr_Ytilde1 = CDF_tau(Yhat.detach(),h,tau)\n",
    "            for z in range(1):\n",
    "                Pr_Ytilde1_Z = CDF_tau(Yhat.detach()[z_batch==z],h,tau)\n",
    "                m_z = z_batch[z_batch==z].shape[0]\n",
    "\n",
    "                Delta_z = Pr_Ytilde1_Z-Pr_Ytilde1\n",
    "                Delta_z_grad = torch.dot(phi((tau-Yhat.detach()[z_batch==z])/h).view(-1), \n",
    "                                          Yhat[z_batch==z].view(-1))/h/m_z\n",
    "                Delta_z_grad -= torch.dot(phi((tau-Yhat.detach())/h).view(-1), \n",
    "                                          Yhat.view(-1))/h/m\n",
    "\n",
    "                if Delta_z.abs() >= delta:\n",
    "                    if Delta_z > 0:\n",
    "                        Delta_z_grad *= lambda_*delta\n",
    "                        cost += Delta_z_grad\n",
    "                    else:\n",
    "                        Delta_z_grad *= -lambda_*delta\n",
    "                        cost += Delta_z_grad\n",
    "                else:\n",
    "                    Delta_z_grad *= lambda_*Delta_z\n",
    "                    cost += Delta_z_grad\n",
    "\n",
    "        # EO_Constraint\n",
    "        elif fairness == 'EO':\n",
    "            for y in [0,1]:\n",
    "                Pr_Ytilde1_Y = CDF_tau(Yhat[y_batch==y].detach(),h,tau)\n",
    "                m_y = y_batch[y_batch==y].shape[0]\n",
    "                for z in range(1):\n",
    "                    Pr_Ytilde1_ZY = CDF_tau(Yhat[(y_batch==y) & (z_batch==z)].detach(),h,tau)\n",
    "                    m_zy = z_batch[(y_batch==y) & (z_batch==z)].shape[0]\n",
    "                    Delta_zy = Pr_Ytilde1_ZY-Pr_Ytilde1_Y\n",
    "                    Delta_zy_grad = torch.dot(\n",
    "                                              phi((tau-Yhat[(y_batch==y) & (z_batch==z)].detach())/h).view(-1), \n",
    "                                              Yhat[(y_batch==y) & (z_batch==z)].view(-1)\n",
    "                                              )/h/m_zy\n",
    "                    Delta_zy_grad -= torch.dot(\n",
    "                                               phi((tau-Yhat[y_batch==y].detach())/h).view(-1), \n",
    "                                               Yhat[y_batch==y].view(-1)\n",
    "                                               )/h/m_y\n",
    "\n",
    "                    if Delta_zy.abs() >= delta:\n",
    "                        if Delta_zy > 0:\n",
    "                            Delta_zy_grad *= lambda_*delta\n",
    "                            cost += Delta_zy_grad\n",
    "                        else:\n",
    "                            Delta_zy_grad *= lambda_*delta\n",
    "                            cost += -lambda_*delta*Delta_zy_grad\n",
    "                    else:\n",
    "                        Delta_zy_grad *= lambda_*Delta_zy\n",
    "                        cost += Delta_zy_grad\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if (torch.isnan(cost)).any():\n",
    "            continue\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        costs.append(cost.item())\n",
    "\n",
    "        # Print the cost per 10 batches\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == len(data_loader):\n",
    "            print('Epoch [{}/{}], Batch [{}/{}], Cost: {:.4f}'.format(epoch+1, n_epochs,\n",
    "                                                                      i+1, len(data_loader),\n",
    "                                                                      cost.item()), end='\\r')\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "def predict(XZ):\n",
    "    Y_hat_ = net(XZ)\n",
    "    Y_hat_[Y_hat_>=0.5] = 1\n",
    "    Y_hat_[Y_hat_ < 0.5] = 0\n",
    "    return Y_hat_\n",
    "\n",
    "\n",
    "# metrics on train set\n",
    "y_hat = predict(XZ_train).flatten()\n",
    "y_hat = y_hat.unsqueeze(1)\n",
    "y_hat_1 = y_hat[Z_train==1]\n",
    "y_hat_0 = y_hat[Z_train==0]\n",
    "y_1 = Y_train[Z_train==1]\n",
    "y_0 = Y_train[Z_train==0]\n",
    "train_results = {}\n",
    "for key in metrics.keys():\n",
    "    train_results[key] = metrics[key](y_hat_1, y_hat_0, y_1, y_0).data.item()\n",
    "\n",
    "# metrics on test set\n",
    "y_hat = predict(XZ_test).flatten()\n",
    "y_hat = y_hat.unsqueeze(1)\n",
    "y_hat_1 = y_hat[Z_test==1]\n",
    "y_hat_0 = y_hat[Z_test==0]\n",
    "y_1 = Y_test[Z_test==1]\n",
    "y_0 = Y_test[Z_test==0]\n",
    "test_results = {}\n",
    "for key in metrics.keys():\n",
    "    test_results[key] = metrics[key](y_hat_1, y_hat_0, y_1, y_0).data.item()\n",
    "\n",
    "train_results['lambda_'] = lambda_\n",
    "test_results['lambda_'] = lambda_\n",
    "results_train.append(train_results)\n",
    "results_test.append(test_results)\n",
    "\n",
    "# df_train = pd.DataFrame(data=results_train)\n",
    "# df_test = pd.DataFrame(data=results_test)\n",
    "\n",
    "# df_train.to_csv('results/{}_zafar_{}_train.csv'.format(args.dataset, 0))\n",
    "\n",
    "# df_test.to_csv('results/{}_zafar_{}_test.csv'.format(args.dataset, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "documentary-grocery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'statistical_parity': 0.21625375747680664,\n",
       "  'statistical_parity_classification': 0.21625381708145142,\n",
       "  'bounded_group_loss_L1': 0.9569469094276428,\n",
       "  'bounded_group_loss_L2': 0.9569469094276428,\n",
       "  'group_fair_expect': 0.21625381708145142,\n",
       "  'l1_dist': 0.21625375747680664,\n",
       "  'l2_dist': 0.21625375747680664,\n",
       "  'MSE': 0.35463258624076843,\n",
       "  'MAE': 0.35463258624076843,\n",
       "  'accuracy': 64.5367431640625,\n",
       "  'lambda_': 0.05}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-charger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
